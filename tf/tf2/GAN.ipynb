{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Generative Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from utils.mnist_dataset import MNIST_Dataset\n",
    "from utils.batch import make_batches_all, make_batches_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(int(tf.__version__[0])==2)  # Use TensorFlow 2\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})  # Figure size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = 'log-GAN'  # For tensorboard, etc.\n",
    "\n",
    "DS_PATH = 'data' # Where de MNIST dataset is located\n",
    "\n",
    "# Hyperparams\n",
    "PARAMS = {\n",
    "    'generator': {\n",
    "        'learning_rate': 0.00001\n",
    "    },\n",
    "    'discriminator': {\n",
    "        'learning_rate': 0.00001\n",
    "    },\n",
    "    'latent_factors': 100,\n",
    "    'epochs': 200,\n",
    "    'batch_size': 64,  # Use an even number\n",
    "    'disc_gen_ratio': 2  # How many times over the generator the discriminator is trained.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash download_mnist.sh {DS_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "        \n",
    "    def __init__(self, ds_path):\n",
    "        self.ds = MNIST_Dataset(ds_path)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _preprocess_samples(x):\n",
    "        \"\"\"x: tensor of shape (-1, 28, 28) representing the images.\n",
    "        \"\"\"\n",
    "        n,w,h = x.shape\n",
    "        # From 28x28 pixels to 32x32\n",
    "        x_32 = np.pad(\n",
    "            x,\n",
    "            pad_width=((0,0),(2,2),(2,2)),\n",
    "            mode='constant',\n",
    "            constant_values=0\n",
    "        )\n",
    "        #x_scaled = (x_32/128)-1 # Scaled -1,1\n",
    "        return x_32.reshape(n,32,32,1)\n",
    "    \n",
    "    \n",
    "    def train_data(self):\n",
    "        x,y = self.ds.get_train()\n",
    "        return (\n",
    "            self._preprocess_samples(x).astype(np.float32),\n",
    "            y\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(latent_factors):\n",
    "    \"\"\" From noise to plausible examples.\n",
    "    INPUT: Noise vector of latent factors (-1,1)\n",
    "    OUTPUT: 32x32x1 (-1,1) grayscale image\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential(\n",
    "        name=\"Generator\",\n",
    "        layers=[\n",
    "            tf.keras.layers.InputLayer(\n",
    "                input_shape=(latent_factors)\n",
    "            ),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"D1\",\n",
    "                units=128,\n",
    "                activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(\n",
    "                momentum=0.8,\n",
    "                name=\"BN1\"\n",
    "            ),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"D2\",\n",
    "                units=256,\n",
    "                activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(\n",
    "                momentum=0.8,\n",
    "                name=\"BN2\"\n",
    "            ),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"D3\",\n",
    "                units=512,\n",
    "                activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(\n",
    "                momentum=0.8,\n",
    "                name=\"BN4\"\n",
    "            ),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"D4\",\n",
    "                units=1024,\n",
    "                activation=tf.math.tanh\n",
    "            ),\n",
    "            # Reshape 120->1x1x120\n",
    "            tf.keras.layers.Reshape(\n",
    "                target_shape=(32,32,1)\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator():\n",
    "    \"\"\"Output the probability for an example to be real.\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential(\n",
    "        name=\"Discriminator\",\n",
    "        layers=[\n",
    "            tf.keras.layers.InputLayer(\n",
    "                input_shape=(32,32,1)\n",
    "            ),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"F3\",\n",
    "                units=512,\n",
    "                activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "            ),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"F4\",\n",
    "                units=256,\n",
    "                activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "            ),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"F5\",\n",
    "                units=1,\n",
    "                activation=tf.math.sigmoid\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan(generator, discriminator):\n",
    "    return tf.keras.Sequential(\n",
    "        name=\"GAN\",\n",
    "        layers=[\n",
    "            tf.keras.layers.InputLayer(\n",
    "                input_shape=(\n",
    "                    generator.layers[0].input.shape[1]\n",
    "                )\n",
    "            ),\n",
    "            generator,\n",
    "            discriminator\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = get_generator(PARAMS['latent_factors'])\n",
    "gen_opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=PARAMS['generator']['learning_rate']\n",
    ")\n",
    "\n",
    "discriminator = get_discriminator()\n",
    "disc_opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=PARAMS['discriminator']['learning_rate']\n",
    ")\n",
    "\n",
    "gan = get_gan(generator, discriminator)\n",
    "\n",
    "generator.summary()\n",
    "discriminator.summary()\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_latent_factors(n):\n",
    "    \"\"\"Noise sample function.\n",
    "    \"\"\"\n",
    "    return tf.random.uniform(shape=(n, PARAMS['latent_factors']), minval=-1.0, maxval=1.0)\n",
    "    #return tf.random.normal(shape=(n, PARAMS['latent_factors']), mean=0.0, stddev=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(epoch, generator, examples=100, dim=(10,10), figsize=(32,32)):\n",
    "    z = sample_latent_factors(examples)\n",
    "    generated_images = generator.predict(z)\n",
    "    generated_images = generated_images.reshape(examples,32,32)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('generator_{}.png'.format(epoch))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, fn_loss, x_train, y_train):\n",
    "    \"\"\"Training step for a model.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_train, training=True)\n",
    "        loss = fn_loss(y_train, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_loss(y_true, y_pred):\n",
    "    \"\"\"Loss function.\n",
    "    Log loss or cross-entropy.\n",
    "    \n",
    "    NOTE: it is possible to use tf.keras.losses.BinaryCrossentropy() alone.\n",
    "    param reduction='SUM_OVER_BATCH_SIZE'  works the same.\n",
    "    \"\"\"\n",
    "    return tf.math.reduce_mean(\n",
    "        tf.keras.losses.binary_crossentropy(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            label_smoothing = 0,  # Not useful since I only need one sided label smoothing\n",
    "            from_logits=False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Tensorboard writer\n",
    "\"\"\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "gen_summary_writer = tf.summary.create_file_writer(LOG_PATH+ '/' + current_time + '/gen')\n",
    "disc_summary_writer = tf.summary.create_file_writer(LOG_PATH+ '/' + current_time + '/disc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset(DS_PATH)\n",
    "x_real, _ = ds.train_data()  # I don't care about labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Precalculate batch labels to gain speed.\n",
    "\"\"\"\n",
    "\n",
    "bs = PARAMS['batch_size']\n",
    "n_batches = math.ceil(len(x_real)*2 / bs) # Real + Fake data / batch_size\n",
    "\n",
    "# Discriminator labels\n",
    "disc_y = tf.concat(\n",
    "    [\n",
    "        tf.fill((bs//2,1), 0.95), # Real. One sided label smoothing\n",
    "        tf.fill((bs//2,1), 0.0)   # Fake\n",
    "    ],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "# Denerator labels\n",
    "gen_y = tf.fill((bs,1), 1.0)  # Do not smooth generator samples!!\n",
    "\n",
    "assert(len(disc_y)==len(gen_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%tensorboard --logdir {train_log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(generator, discriminator, val_data_real, val_data_fake):\n",
    "    n = len(val_data_real)\n",
    "    y_true = tf.concat([tf.fill((n,1), 1.0),tf.fill((n,1), 0.0)], axis=0)\n",
    "    y_pred = discriminator( tf.concat([val_data_real, val_data_fake ],axis=0) )\n",
    "    y_true_bin = y_true.numpy().flatten()\n",
    "    y_pred_bin = np.where(y_pred.numpy().flatten()>=0.5, 1, 0)\n",
    "\n",
    "    # Metrics\n",
    "    cf = sklearn.metrics.confusion_matrix(y_true, y_pred_bin)\n",
    "    return {\n",
    "        'disc_loss': fn_loss(y_true, y_pred),\n",
    "        'gen_loss': fn_loss(tf.fill((n,1), 0.0), discriminator(val_data_fake)),\n",
    "        'acc': sklearn.metrics.accuracy_score(y_true, y_pred_bin),\n",
    "        'fpr': cf[0,1] / (cf[0,1]+cf[0,0]), # False positive ratio\n",
    "        'cm': cf\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Training loop.\n",
    "\"\"\"\n",
    "\n",
    "# Generator for real data, random sampling strategy\n",
    "x_real_g=make_batches_random(\n",
    "    x=x_real,\n",
    "    y=None,\n",
    "    batch_size=bs//2,\n",
    "    stop_after_epoch=False\n",
    ")\n",
    "\n",
    "# Validation data\n",
    "val_data_real=np.take(  # Real samples\n",
    "    a=x_real,\n",
    "    indices=np.random.randint(\n",
    "        low=0,\n",
    "        high=len(x_real), \n",
    "        size=256\n",
    "    ),\n",
    "    axis=0\n",
    ")\n",
    "val_params_fake = sample_latent_factors(256)\n",
    "\n",
    "\n",
    "for epoch in range(0, PARAMS['epochs']):  \n",
    "    # Batch\n",
    "    with tqdm_notebook(total=n_batches, unit='batch', desc=\"Epoch: {} \".format(epoch)) as pbar:\n",
    "        for n_batch in range(n_batches):\n",
    "            for i in range(PARAMS['disc_gen_ratio']):\n",
    "                # DISCRIMINATOR\n",
    "                _, disc_x_real, _ = x_real_g.__next__()\n",
    "                disc_x = tf.concat([\n",
    "                    disc_x_real,\n",
    "                    generator(sample_latent_factors(bs//2))  # Fake data\n",
    "                ], axis=0)\n",
    "                discriminator.trainable=True\n",
    "                disc_b_loss = train_step(\n",
    "                    model=discriminator,\n",
    "                    optimizer=disc_opt,\n",
    "                    fn_loss=fn_loss,\n",
    "                    x_train=disc_x,\n",
    "                    y_train=disc_y\n",
    "                )    \n",
    "            # GENERATOR\n",
    "            discriminator.trainable=False\n",
    "            gen_b_loss = train_step(\n",
    "                model=gan,  # Latent params -> Generator -> image -> Discriminator -> Probability\n",
    "                optimizer=gen_opt,\n",
    "                fn_loss=fn_loss,\n",
    "                x_train=sample_latent_factors(bs),\n",
    "                y_train=gen_y\n",
    "            )\n",
    "\n",
    "            #Update progress bar\n",
    "            pbar.set_postfix(disc_loss=disc_b_loss.numpy(), gen_loss=gen_b_loss.numpy())\n",
    "            pbar.update(1)\n",
    "\n",
    "        \n",
    "    # EPOCH METRICS\n",
    "    val_data_fake = generator(val_params_fake)\n",
    "    val_metrics = evaluate(generator, discriminator, val_data_real, val_data_fake)\n",
    "    for m in ['disc_loss','acc','gen_loss','fpr','cm']:\n",
    "        print(\"{}\\t{}\".format(m, val_metrics[m])\n",
    "    \n",
    "    with disc_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', val_metrics[\"disc_loss\"], step=epoch)\n",
    "        tf.summary.scalar('acc', val_metrics[\"acc\"], step=epoch)\n",
    "    with gen_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', val_metrics[\"gen_loss\"], step=epoch)\n",
    "        tf.summary.scalar('acc', val_metrics[\"fpr\"], step=epoch)\n",
    "        tf.summary.image(\"Training data\", val_data_fake, step=epoch, max_outputs=25)\n",
    "    \n",
    "    if epoch%2 == 0:\n",
    "        plot_generated_images(epoch+1, generator) # Output generated images\n",
    "        \n",
    "    if epoch%5 ==0:\n",
    "        pass\n",
    "        # TODO: Save model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample = generator(sample_latent_factors(1))[0].numpy().reshape(32,32)\n",
    "sample = disc_x[42].reshape(32,32)\n",
    "print(\"Disc prob: {}\".format(discriminator(sample.reshape(-1,32,32,1)).numpy()[0][0]))\n",
    "\n",
    "plt.imshow(\n",
    "    sample, \n",
    "    cmap='gray'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator(np.take(  # Real samples\n",
    "                    a=x_real,\n",
    "                    indices=np.random.randint(\n",
    "                        low=0,\n",
    "                        high=n_data, \n",
    "                        size=256\n",
    "                    ),\n",
    "                    axis=0\n",
    "                ),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_val_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_val_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate fake samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = generator(sample_latent_factors(1))[0].numpy().reshape(32,32)\n",
    "print(\"Disc prob: {}\".format(discriminator(sample.reshape(-1,32,32,1)).numpy()[0][0]))\n",
    "\n",
    "plt.imshow(\n",
    "    sample, \n",
    "    cmap='gray'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "with open('saved_models/GAN01/model.json', 'w+') as fd:\n",
    "    fd.writelines(model.to_json())\n",
    "model.save_weights('saved_models/01-k/weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. <a name=\"bib-web-gantf\"></a>[Building a simple Generative Adversarial Network (GAN) using TensorFlow](https://blog.paperspace.com/implementing-gans-in-tensorflow/)\n",
    "2. <a name=\"bib-web-adversarialtf\"></a>[Generative Adversarial Nets in TensorFlow](https://wiseodd.github.io/techblog/2016/09/17/gan-tensorflow/)\n",
    "3. <a name=\"bib-web-gankeras\"></a>[Generative Adversarial Network(GAN) using Keras](https://medium.com/datadriveninvestor/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3)\n",
    "4. <a name=\"bib-web-poolstride\"></a>[Pooling VS Striding - Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806)\n",
    "5. <a name=\"bib-web-collapse1\"></a>[Mode collapse: GAN — Why it is so hard to train Generative Adversarial Networks!](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)\n",
    "6. <a name=\"bib-web-collapse2\"></a>[Mode collapse: What does it mean if all produced images of a GAN look the same?](https://www.quora.com/What-does-it-mean-if-all-produced-images-of-a-GAN-look-the-same)\n",
    "7. <a name=\"bib-vid-gans\"></a>[NIPS 2016 - Generative Adversarial Networks - Ian Goodfellow](https://www.youtube.com/watch?v=AJVyzd0rqdc)\n",
    "  1. [On divergence](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=52m10s)\n",
    "  1. [On labeled/conditioned GANS](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=1h09m50s)\n",
    "  1. [On mode collapse](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=1h31m53s)\n",
    "  1. [One sided label smoothing](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=1h11m34s)\n",
    "  1. [Question: GANs vs VAEs](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=37m10s)\n",
    "  1. [Question: Sampling distributions uniform VS Norma](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=37m57s)\n",
    "  1. [Question: mode collapse/same sample](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=35m47s)\n",
    "8.  <a name=\"bib-web-tbstarted\"></a>[Get started with TensorBoard](https://www.tensorflow.org/tensorboard/r2/get_started#using_tensorboard_with_other_methods)\n",
    "9. <a name=\"bib-web-codegan1\"></a>[Github: PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py)\n",
    "10. [TQDM (status bar library)](https://tqdm.github.io/)\n",
    "11. [Inside TensorFlow: Summaries and TensorBoard](https://www.youtube.com/watch?v=OI4cskHUslQ)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
