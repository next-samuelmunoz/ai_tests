{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Generative Networks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IDEA:\n",
    "\n",
    "\n",
    "\n",
    "Test a Condicional Adversarial Generative Network\n",
    "* [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)\n",
    "* [GAN — CGAN & InfoGAN (using labels to improve GAN)](https://medium.com/@jonathan_hui/gan-cgan-infogan-using-labels-to-improve-gan-8ba4de5f9c3d)\n",
    "* [What is the difference between Generative Adversarial Networks and Autoencoders?](https://www.quora.com/What-is-the-difference-between-Generative-Adversarial-Networks-and-Autoencoders)\n",
    "* [DCGAN in Tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)\n",
    " \n",
    "\n",
    "* [Conditional Generative Adversarial Nets in TensorFlow](https://wiseodd.github.io/techblog/2016/12/24/conditional-gan-tensorflow/)\n",
    "* [Image Completion with Deep Learning in TensorFlow](http://bamos.github.io/2016/08/09/deep-completion/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(int(tf.__version__[0])==2)  # Use TensorFlow 2\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})  # Figure size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = 'log-GAN'  # For tensorboard, etc.\n",
    "\n",
    "DS_PATH = 'data' # Where de MNIST dataset is located\n",
    "\n",
    "# Hyperparams\n",
    "PARAMS = {\n",
    "    'generator': {\n",
    "        'learning_rate': 0.0002\n",
    "    },\n",
    "    'discriminator': {\n",
    "        'learning_rate': 0.0002\n",
    "    },\n",
    "    'latent_factors': 100,\n",
    "    'epochs': 200,\n",
    "    'batch_size': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "class MNIST_Dataset():\n",
    "    \"\"\"Based on the tf.data\n",
    "    # TODO: replace the StandarScaler for a Tensorflow version. See: tf.nn.moments\n",
    "    \"\"\"\n",
    "    def __init__(self, ds_path):\n",
    "        \"\"\"ds_path is the folder where the 4 .gz files are located\n",
    "        \"\"\"\n",
    "        self.FILES = {\n",
    "            'train': ds_path+\"/train-images-idx3-ubyte.gz\",\n",
    "            'train_labels': ds_path+\"/train-labels-idx1-ubyte.gz\",\n",
    "            'test': ds_path+\"/t10k-images-idx3-ubyte.gz\",\n",
    "            'test_labels': ds_path+\"/t10k-labels-idx1-ubyte.gz\"\n",
    "        }\n",
    "        \n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _read_idx(filename):\n",
    "        \"\"\"Read a tensor from a file in idx format.\n",
    "        \"\"\"\n",
    "        with gzip.open(filename, 'rb') as fd:\n",
    "            _, data_type, dims = struct.unpack('>HBB', fd.read(4))\n",
    "            shape = tuple(\n",
    "                struct.unpack('>I', fd.read(4))[0]\n",
    "                for d\n",
    "                in range(dims)\n",
    "            )\n",
    "            return np.frombuffer(fd.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def _preprocess(x):\n",
    "        \"\"\"x: tensor of shape (-1, 28, 28) representing the images.\n",
    "        \"\"\"\n",
    "        n,w,h = x.shape\n",
    "        # From 28x28 pixels to 32x32\n",
    "        x_32 = np.pad(\n",
    "            x,\n",
    "            pad_width=((0,0),(2,2),(2,2)),\n",
    "            mode='constant',\n",
    "            constant_values=0\n",
    "        )\n",
    "        #x_scaled = (x_32/128)-1 # Scaled -1,1\n",
    "        return x_32.reshape(n,32,32,1)\n",
    "    \n",
    "    \n",
    "    def train_data(self): \n",
    "        return (\n",
    "            self._preprocess(self._read_idx(self.FILES['train'])),\n",
    "            self._read_idx(self.FILES['train_labels'])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MNIST_Dataset(DS_PATH)\n",
    "x_real, _ = ds.train_data()  # I don't care about labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(latent_factors):\n",
    "    \"\"\" From noise to plausible examples.\n",
    "    INPUT: Noise vector of latent factors (-1,1)\n",
    "    OUTPUT: 32x32x1 (-1,1) grayscale image\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential(\n",
    "        name=\"Generator\",\n",
    "        layers=[\n",
    "            tf.keras.layers.InputLayer(\n",
    "                input_shape=(latent_factors)\n",
    "            ),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"D1\",\n",
    "                units=128,\n",
    "                activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(\n",
    "                momentum=0.8,\n",
    "                name=\"BN1\"\n",
    "            ),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"D2\",\n",
    "                units=256,\n",
    "                activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(\n",
    "                momentum=0.8,\n",
    "                name=\"BN2\"\n",
    "            ),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"D3\",\n",
    "                units=512,\n",
    "                activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(\n",
    "                momentum=0.8,\n",
    "                name=\"BN4\"\n",
    "            ),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"D4\",\n",
    "                units=1024,\n",
    "                activation=tf.math.tanh\n",
    "            ),\n",
    "            # Reshape 120->1x1x120\n",
    "            tf.keras.layers.Reshape(\n",
    "                target_shape=(32,32,1)\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator():\n",
    "    \"\"\"Output the probability for an example to be real.\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential(\n",
    "        name=\"Discriminator\",\n",
    "        layers=[\n",
    "            tf.keras.layers.InputLayer(\n",
    "                input_shape=(32,32,1)\n",
    "            ),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"F3\",\n",
    "                units=512,\n",
    "                activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "            ),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"F4\",\n",
    "                units=256,\n",
    "                activation=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "            ),\n",
    "            tf.keras.layers.Dense(  \n",
    "                name=\"F5\",\n",
    "                units=1,\n",
    "                activation=tf.math.sigmoid\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan(generator, discriminator):\n",
    "    return tf.keras.Sequential(\n",
    "        name=\"GAN\",\n",
    "        layers=[\n",
    "            tf.keras.layers.InputLayer(\n",
    "                input_shape=(\n",
    "                    generator.layers[0].input.shape[1]\n",
    "                )\n",
    "            ),\n",
    "            generator,\n",
    "            discriminator\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = get_generator(PARAMS['latent_factors'])\n",
    "gen_opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "discriminator = get_discriminator()\n",
    "disc_opt = tf.keras.optimizers.Adam() \n",
    "\n",
    "gan = get_gan(generator, discriminator)\n",
    "\n",
    "generator.summary()\n",
    "discriminator.summary()\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_latent_factors(n):\n",
    "    \"\"\"Noise sample function.\n",
    "    \"\"\"\n",
    "    return tf.random.uniform(shape=(n, PARAMS['latent_factors']), minval=-1.0, maxval=1.0)\n",
    "    #return tf.random.normal(shape=(n, PARAMS['latent_factors']), mean=0.0, stddev=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(epoch, generator, examples=100, dim=(10,10), figsize=(32,32)):\n",
    "    z = sample_latent_factors(examples)\n",
    "    generated_images = generator.predict(z)\n",
    "    generated_images = generated_images.reshape(examples,32,32)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('generator_{}.png'.format(epoch))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, fn_loss, x_train, y_train):\n",
    "    \"\"\"Training step for a model.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_train, training=True)\n",
    "        loss = fn_loss(y_train, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_loss(y_true, y_pred):\n",
    "    \"\"\"Loss function.\n",
    "    Log loss or cross-entropy.\n",
    "    \"\"\"\n",
    "    return tf.math.reduce_mean(\n",
    "        tf.keras.losses.binary_crossentropy(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            label_smoothing = 0  # Not useful since I only need one sided label smoothing\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Tensorboard writer\n",
    "\"\"\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = LOG_PATH+ '/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Precalculated data to speedup calculations.\n",
    "\"\"\"\n",
    "# Real data\n",
    "n_data = x_real.shape[0]\n",
    "bs = PARAMS['batch_size']\n",
    "n_real = n_fake = bs//2\n",
    "x_real = np.concatenate([\n",
    "    x_real,\n",
    "    np.take(  # Data to fill last batch\n",
    "        a=x_real,\n",
    "        indices=np.random.randint(\n",
    "            low=0,\n",
    "            high=n_data, \n",
    "            size=n_data%PARAMS['batch_size']\n",
    "        ),\n",
    "        axis=0\n",
    "    )\n",
    "])\n",
    "n_data = x_real.shape[0]\n",
    "assert(n_data % bs == 0)\n",
    "batches = ((n_data*2)//bs)  # Batches per epoch (real+fake data)\n",
    "\n",
    "# Batch labels for the discriminator\n",
    "disc_y = tf.concat(\n",
    "    [\n",
    "        tf.fill((n_real,), 0.90), # Real. One sided label smoothing\n",
    "        tf.fill((n_fake,), 0.0)   # Fake\n",
    "    ],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "# Batch labels for the generator\n",
    "gen_y = tf.fill((bs,), 0.90)  # Objective is to fool the discriminator, one sided label smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%tensorboard --logdir {train_log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Training loop.\n",
    "\"\"\"\n",
    "\n",
    "PARAMS['epochs'] = 100\n",
    "\n",
    "\n",
    "for epoch in range(0, PARAMS['epochs']):\n",
    "    print(\"Epoch: {}\".format(epoch+1))\n",
    "    # Discriminator data\n",
    "    np.random.shuffle(x_real)\n",
    "    disc_x_fake = generator(sample_latent_factors(n_data))\n",
    "    # Generator data\n",
    "    gen_x_fake = sample_latent_factors(n_data*2)  # Same amount of data as for the discriminator\n",
    "   \n",
    "    for batch in tqdm(range(batches)):\n",
    "        # DISCRIMINATOR\n",
    "        discriminator.trainable=True\n",
    "        train_step(\n",
    "            model=discriminator,\n",
    "            optimizer=disc_opt,\n",
    "            fn_loss=fn_loss,\n",
    "            x_train=np.concatenate([\n",
    "                x_real[batch*(n_real) : (batch+1)*n_real],      # Real samples\n",
    "                disc_x_fake[batch*(n_fake) : (batch+1)*n_fake]  # Fake samples\n",
    "            ]),\n",
    "            y_train=disc_y\n",
    "        )        \n",
    "        # GENERATOR\n",
    "        discriminator.trainable=False\n",
    "        train_step(\n",
    "            model=gan,  # Latent params -> Generator -> image -> Discriminator -> Probability\n",
    "            optimizer=gen_opt,\n",
    "            fn_loss=fn_loss,\n",
    "            x_train=gen_x_fake[batch*bs : (batch+1)*bs],\n",
    "            y_train=gen_y\n",
    "        )\n",
    "        \n",
    "    # EPOCH METRICS\n",
    "    random_noise = sample_latent_factors(256)\n",
    "    disc_loss = tf.metrics.binary_accuracy(\n",
    "        y_true=tf.concat(\n",
    "            [\n",
    "                tf.fill((256,), 1.0), # Real\n",
    "                tf.fill((256,), 0.0)  # Fake\n",
    "            ],\n",
    "            axis=0\n",
    "        ),\n",
    "        y_pred=tf.reshape(\n",
    "            tensor=discriminator(\n",
    "                tf.concat([\n",
    "                    np.take(  # Real samples\n",
    "                        a=x_real,\n",
    "                        indices=np.random.randint(\n",
    "                            low=0,\n",
    "                            high=n_data, \n",
    "                            size=256\n",
    "                        ),\n",
    "                        axis=0\n",
    "                    ),\n",
    "                    generator(random_noise) # Fake samples\n",
    "                ],\n",
    "                axis=0\n",
    "            )),\n",
    "            shape=(-1,)\n",
    "        )\n",
    "    )\n",
    "    gen_loss = tf.metrics.binary_accuracy(\n",
    "        y_true=tf.fill((256,), 1.0),\n",
    "        y_pred=tf.reshape(gan(random_noise), (-1,))\n",
    "    )\n",
    "    \n",
    "    print(\"Discriminator Acc: {}\".format(disc_loss))\n",
    "    print(\"Generator Acc: {}\".format(gen_loss))\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n",
    "        tf.summary.scalar('gen_loss', gen_loss, step=epoch)\n",
    "    \n",
    "    if epoch%2 == 0:\n",
    "        plot_generated_images(epoch+1, generator) # Output generated images\n",
    "        # TODO: Save model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate fake samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = generator(sample_latent_factors(1))[0].numpy().reshape(32,32)\n",
    "print(\"Disc prob: {}\".format(discriminator(sample.reshape(-1,32,32,1)).numpy()[0][0]))\n",
    "\n",
    "plt.imshow(\n",
    "    sample, \n",
    "    cmap='gray'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "with open('saved_models/GAN01/model.json', 'w+') as fd:\n",
    "    fd.writelines(model.to_json())\n",
    "model.save_weights('saved_models/01-k/weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. <a name=\"bib-web-gantf\"></a>[Building a simple Generative Adversarial Network (GAN) using TensorFlow](https://blog.paperspace.com/implementing-gans-in-tensorflow/)\n",
    "2. <a name=\"bib-web-adversarialtf\"></a>[Generative Adversarial Nets in TensorFlow](https://wiseodd.github.io/techblog/2016/09/17/gan-tensorflow/)\n",
    "3. <a name=\"bib-web-gankeras\"></a>[Generative Adversarial Network(GAN) using Keras](https://medium.com/datadriveninvestor/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3)\n",
    "4. <a name=\"bib-web-poolstride\"></a>[Pooling VS Striding - Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806)\n",
    "5. <a name=\"bib-web-collapse1\"></a>[Mode collapse: GAN — Why it is so hard to train Generative Adversarial Networks!](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)\n",
    "6. <a name=\"bib-web-collapse2\"></a>[Mode collapse: What does it mean if all produced images of a GAN look the same?](https://www.quora.com/What-does-it-mean-if-all-produced-images-of-a-GAN-look-the-same)\n",
    "7. <a name=\"bib-vid-gans\"></a>[NIPS 2016 - Generative Adversarial Networks - Ian Goodfellow](https://www.youtube.com/watch?v=AJVyzd0rqdc)\n",
    "  1. [On divergence](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=52m10s)\n",
    "  1. [On labeled/conditioned GANS](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=1h09m50s)\n",
    "  1. [On mode collapse](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=1h31m53s)\n",
    "  1. [One sided label smoothing](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=1h11m34s)\n",
    "  1. [Question: GANs vs VAEs](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=37m10s)\n",
    "  1. [Question: mode collapse/same sample](https://www.youtube.com/watch?v=AJVyzd0rqdc&t=35m47s)\n",
    "8.  <a name=\"bib-web-tbstarted\"></a>[Get started with TensorBoard](https://www.tensorflow.org/tensorboard/r2/get_started#using_tensorboard_with_other_methods)\n",
    "9. <a name=\"bib-web-codegan1\"></a>[Github: PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
